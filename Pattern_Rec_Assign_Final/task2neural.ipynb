{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **ΠΡΟΒΛΗΜΑ ΛΟΓΙΣΤΙΚΗΣ ΠΑΛΙΝΔΡΟΜΗΣΗΣ – ΠΡΟΣΔΕΣΗ ΧΗΜΙΚΩΝ ΜΟΡΙΩΝ ΣΕ ΥΠΟΔΟΧΕΙΣ**\n",
        "\n",
        "Σε αυτό το πρόβλημα υφίστανται δυαδικά και συνεχή χαρακτηριστικά. Το πρόβλημα ήδη ανάγεται ενδεικτικά στη χρήση δεντρικών ή νευρικών μοντέλων ή ακόμα και συνδυασμό τους. Στη παρούσα άσκηση χρησιμοποιείται ένα νευρωνικό δίκτυο.\n",
        "Η μόνη προ-επεξεργασία που υφίστανται τα συνεχή δεδομένα είναι η κανονικοποίησή τους (για τα συνεχή χαρακτηριστικά), ενώ σε σχέση με το προηγούμενο πρόβλημα δεν έχουμε πληροφορίες που να υποδεικνύουν τη χρήση μείωσης διαστάσεων για την απλοποίηση και ταχύτερη σύγκλιση του μοντέλου.  \n",
        "Μοναδικό ζήτημα που τίθεται είναι η ανισοκατανομή του πλήθος δεδομένων των 2 κλάσεων,0 και 1, με ποσοστά 33.5% και 66.5% αντιστοίχως. Αυτό συνεπάγεται πως η μετρική της ακρίβειας (accuracy) δεν αρκεί για να προσδιορίσει τη πραγματική συμπεριφορά του μοντέλου, καθώς μπορεί να υπάρχει μεγαλύτερη τάση σωστής ταξινόμησης της επικρατέστερης –αριθμητικά- κλάσης με αποτέλεσμα να δίνεται ψευδώς η αίσθηση πως ο ταξινομητής λειτουργεί αμερόληπτα και το ίδιο αποδοτικά και για τις 2 κατηγορίες.\n"
      ],
      "metadata": {
        "id": "DTkejjHm9yQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, average_precision_score\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "\n",
        "# Load Data\n",
        "train_features = pd.read_csv(\"Train_Features.csv\")\n",
        "train_labels = pd.read_csv(\"Train_Labels.csv\")\n",
        "test_features = pd.read_csv(\"Test_Features.csv\")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# See the percentage of class 1 to determine dataset imbalances\n",
        "print(f\"Samples that binded to the receptor are {np.sum(train_labels == 1) / len(train_labels) * 100}%\")\n",
        "\n",
        "def preprocess_data(features):\n",
        "    # Separate continuous and binary features\n",
        "    continuous = features.iloc[:, :1425]\n",
        "    binary = features.iloc[:, 1425:]\n",
        "    # Scale only continuous features (duh)\n",
        "    continuous_scaled = scaler.fit_transform(continuous)\n",
        "\n",
        "    return np.hstack((continuous_scaled, binary.values))\n",
        "\n",
        "# Preprocess training data\n",
        "X = preprocess_data(train_features)\n",
        "y = train_labels.values.ravel()  # Flatten the 2D vector\n",
        "\n",
        "# Split data for validation / stratify to keep same distribution in train and validation / not used eventually\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
        "\n",
        "# Create DataLoader for batching\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKkL969Tqrv-",
        "outputId": "70ee5a92-2c4c-4556-86b7-304a090b4f24"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samples that binded to the receptor are 1    66.427289\n",
            "dtype: float64%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
            "  return reduction(axis=axis, out=out, **passkwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Η δομή καθορίστηκε με γενικές κατευθυντήριες και πειραματισμό.\n",
        "   Προτιμήθηκαν περισσότερα layers με λιγότερους νευρώνες σε καθένα.\n",
        "\n",
        "- Κάθε layer είναι Dense με ReLU για ενεργοποίηση\n",
        "\n",
        "Batch normalization για ταχύτερη σύγκλιση,περιορισμό  \n",
        "  μεγάλων εξόδων, αποφυγή overfitting & dropout neurons,\n",
        "  για καλύτερη απόδοση στο validation & testing phase πριν το τέλος\n",
        "  δικτύου πάλι για αποφυγή overfitting\n",
        "\n",
        "- Σιγμοειδής στην έξοδο\n",
        "\n",
        "Φτιάχνεται ένα πλέγμα από παραμέτρους στη συνέχεια το οποίο θα χρησιμοποιηθεί για να βρεθεί το βέλτιστο set που δίνει το καλύτερο μοντέλο βάση μετρικής/μετρικών που υπολογίζονται.\n"
      ],
      "metadata": {
        "id": "QYBgQkAg_bwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define the Neural Network\n",
        "class ReceptorBindPredictor(nn.Module):\n",
        "    def __init__(self, dr):\n",
        "        super(ReceptorBindPredictor, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "          nn.Linear(X_train.shape[1], 1),\n",
        "          #nn.ReLU(),\n",
        "          #nn.Linear(64, 64),\n",
        "          #nn.ReLU(),\n",
        "          #nn.Linear(64, 64),\n",
        "          #nn.ReLU(),\n",
        "          #nn.Linear(64, 32),\n",
        "          #nn.ReLU(),\n",
        "          #nn.Dropout(dr),\n",
        "          #nn.BatchNorm1d(32),\n",
        "          nn.Linear(1, 1),\n",
        "          nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Calculate combined score\n",
        "def calculate_combined_score(auc, accuracy, loss, auprc):\n",
        "    combined_score = (0.26 * auc +\n",
        "                      0.23 * accuracy -\n",
        "                      0.24 * loss +\n",
        "                      0.27 * auprc)\n",
        "\n",
        "    return combined_score\n",
        "\n",
        "# Define the grid of hyperparameters\n",
        "param_grid = {\n",
        "    'lr': [0.001, 0.005],\n",
        "    'dropout_rate': [0.2, 0.3, 0.4],\n",
        "    'num_epochs': [50],\n",
        "    'batch_size': [32, 64]\n",
        "}\n"
      ],
      "metadata": {
        "id": "q_YSeymj_c7r"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Για την εξασφάλιση σταθερής απόδοσης γίνεται και πάλι k cross validation, έτσι ώστε το σύστημα να είναι αξιόπιστο και η εκπαίδευσή του να μην εξαρτάται από τον τυχαίο διαμοιρασμό των batches, αλλά και των διαχωρισμό των δεδομένων σε training και validation set.\n",
        "Οι μετρικές που υπολογίζονται είναι, το σφάλμα με Binary Cross\n",
        "   Entropy, accuracy, AUC και AUPRC.\n",
        "Εδώ κάθε μετρική φέρει διαφορετική βαρύτητα,\n",
        "   λόγω των δεδομένων του προβλήματος.\n",
        "Γίνεται μια συνάρτηση που συμψηφίζει τις μετρικές. Τα βάρη\n",
        "   ρυθμίζονται εμπειρικά, αλλά και σύμφωνα με τις σχετικές και\n",
        "   απόλυτες αποκλίσεις κάθε μετρικής.\n",
        "   \n",
        "   Αυτή η τεχνική δεν βρέθηκε σε κάποια βιβλιογραφική πηγή, αλλά ήταν μια τελείως πειραματική μέθοδος που θυμίζει όμοιες τεχνικές που χρησιμοποιούνται στη μηχανική εντός και εκτός του πεδίου του AI (όπως είναι το weighted voting στα ensemble μοντέλα, αλλά και σε εφαρμογές φίλτρων για noise reduction αντίστοιχα)\n",
        "\n"
      ],
      "metadata": {
        "id": "awXALz5YAg6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "best_combined_score = -1  # Initialize with lowest value\n",
        "best_model = None\n",
        "best_params = None\n",
        "best_val_loss = None\n",
        "best_val_auc = 0\n",
        "best_val_acc = 0\n",
        "best_val_auprc = 0\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True)  # 5-fold cross-validation\n",
        "\n",
        "for parameters in ParameterGrid(param_grid):\n",
        "    fold_metrics = []  # Track metrics for each fold\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X, y):\n",
        "        # Split data\n",
        "        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
        "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
        "\n",
        "        # Create DataLoaders\n",
        "        train_dataset = TensorDataset(torch.tensor(X_train_fold, dtype=torch.float32),\n",
        "                                      torch.tensor(y_train_fold, dtype=torch.float32))\n",
        "        val_dataset = TensorDataset(torch.tensor(X_val_fold, dtype=torch.float32),\n",
        "                                    torch.tensor(y_val_fold, dtype=torch.float32))\n",
        "        train_loader = DataLoader(train_dataset, batch_size=parameters['batch_size'], shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=parameters['batch_size'])\n",
        "\n",
        "        # Initialize and train model\n",
        "        model = ReceptorBindPredictor(parameters['dropout_rate'])\n",
        "        optimizer = optim.Adam(model.parameters(), lr=parameters['lr'])\n",
        "\n",
        "        for epoch in range(parameters['num_epochs']):\n",
        "            model.train()\n",
        "            for X_batch, y_batch in train_loader:\n",
        "                optimizer.zero_grad() # To prevent gradient contamination\n",
        "                predictions = model(X_batch).squeeze() # Feed data and get predictions ε[0,1]\n",
        "                loss = criterion(predictions, y_batch) # to assess error\n",
        "                loss.backward() # Update weights based on gradients\n",
        "                optimizer.step() # Update model parameters\n",
        "\n",
        "        # Validation metrics\n",
        "        model.eval() # Switch to evaluation mode - disable dropout for consistent results / enable batchNorm for running statistics\n",
        "        val_predictions = []\n",
        "        val_targets = []\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in val_loader:\n",
        "                y_pred = model(X_batch).squeeze()\n",
        "                val_predictions.extend(y_pred.numpy())\n",
        "                val_targets.extend(y_batch.numpy())\n",
        "\n",
        "        val_auc = roc_auc_score(val_targets, val_predictions)\n",
        "        val_accuracy = accuracy_score(val_targets, (np.array(val_predictions) > 0.5).astype(int))\n",
        "        val_loss = criterion(torch.tensor(val_predictions), torch.tensor(val_targets)).item()\n",
        "        val_auprc = average_precision_score(val_targets, val_predictions)\n",
        "\n",
        "        # Store metrics\n",
        "        fold_metrics.append({\n",
        "            'auc': val_auc,\n",
        "            'accuracy': val_accuracy,\n",
        "            'loss': val_loss,\n",
        "            'auprc': val_auprc\n",
        "        })\n",
        "\n",
        "    # Average metrics across folds\n",
        "    avg_auc = np.mean([m['auc'] for m in fold_metrics])\n",
        "    avg_accuracy = np.mean([m['accuracy'] for m in fold_metrics])\n",
        "    avg_loss = np.mean([m['loss'] for m in fold_metrics])\n",
        "    avg_auprc = np.mean([m['auprc'] for m in fold_metrics])\n",
        "    combined_score = calculate_combined_score(avg_auc, avg_accuracy, avg_loss, avg_auprc)\n",
        "\n",
        "    # Update best model if necessary\n",
        "    if combined_score > best_combined_score:\n",
        "        best_combined_score = combined_score\n",
        "        best_model = model\n",
        "        best_params = parameters\n",
        "        best_val_auc = avg_auc\n",
        "        best_val_acc = avg_accuracy\n",
        "        best_val_loss = avg_loss\n",
        "        best_val_auprc = avg_auprc\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "print(f\"Best model has Val Loss: {best_val_loss/len(val_loader):.4f}, Val AUC: {best_val_auc:.4f}, Val AUPRC: {val_auprc:.4f}, Val Accuracy: {best_val_acc:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERX9L2Ua_qFK",
        "outputId": "9d9d2fa1-c6c9-49cd-c480-25f93a36e2a0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'batch_size': 64, 'dropout_rate': 0.3, 'lr': 0.001, 'num_epochs': 50}\n",
            "Best model has Val Loss: 0.0643, Val AUC: 0.9557, Val AUPRC: 0.9685, Val Accuracy: 0.9093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Τέλος παράγονται τα εκτιμώμωνα labels του προβλήματος και υλοποιείται μια συνάρτηση που για κάθε επόμενο test signal που μπορεί να δωθεί, να γίνεται απευθείας η εκτίμηση της κλάσης του."
      ],
      "metadata": {
        "id": "5PL0zbAiCmFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess test data\n",
        "test_features_aligned = test_features.reindex(columns=train_features.columns, fill_value=0)\n",
        "X_test = preprocess_data(test_features_aligned)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "\n",
        "# Generate submission file\n",
        "def generate_submission_file(model, X_test_tensor, file_name):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_probabilities = model(X_test_tensor).squeeze().numpy() # Make it 2D and numpy\n",
        "    test_predictions = (test_probabilities > 0.5).astype(int)\n",
        "    submission = pd.DataFrame({\n",
        "        'predicted_label': test_predictions,\n",
        "        'prediction_score': test_probabilities\n",
        "    })\n",
        "    submission.to_csv(file_name, index=False)\n",
        "\n",
        "output_file = \"test_predictions_task2_58352.csv\"\n",
        "generate_submission_file(best_model, X_test_tensor, output_file)"
      ],
      "metadata": {
        "id": "Xm0d9qzl_z1h"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ΣΥΜΠΕΡΑΣΜΑΤΑ ΚΑΙ ΠΑΡΑΔΟΧΕΣ**\n",
        "\n",
        "Εντοπίστηκαν αρκετά προβλήματα εκ του αποτελέσματος, που δεν επέτρεψαν το μοντέλο να εκπαιδευτεί σωστά και οδήγησαν τις μετρικές να επιστρέφουν ψευδώς οπτιμιστικά αποτελέσματα.\n",
        "\n",
        "1) Μηδαμινό αλλά υπαρκτό σφάλμα ήταν το ότι ο τρόπος που διαβάζονταν τα δεδομένα έπιαναν τη πρώτη σειρά από κάθε CSV ως επικεφαλίδα και όχι ως δείγμα. Αυτό διορθωνόταν με ένα flag εντός της συνάρτησης (header=None)\n",
        "\n",
        "2) Υπήρχε κατά κάποιο τρόπο data leakage, καθώς γινόταν normalisation σε όλο το training set, αλλά όχι σε κάθε fold μεμονωμένα, με αποτέλεσμα να δίνονται πληροφορίες για τη μέση τιμή και τυπική απόκλιση των χαρακτηριστικών από το - διαφορετικό κάθε φορά- validation set, οπότε και τα αποτελέσματα κατά το validation phase ήταν ψευδή.\n",
        "\n",
        "3) Δεν έγινε χρήση του stratify στην KFOLD, (π.χ χρήση της StratifiedKFold), οπότε το class imbalance δεν ήταν κάθε στιγμή της εκπαίδευσης κοινό.\n",
        "\n",
        "4) Η αποφυγή χρήσης PCA (αν και δικαιολογημένη σε πρώτη όψη, διότι δεν είχαμε κάποια σημασιολογική πληροφορία που να υποδείξει πως η μείωση διαστάσεων θα ήταν θεμιτή), μπορεί να είχε ως αποτέλεσμα να εκπαιδεύεται το μοντέλο με δεδομένα τα οποία να παρουσιάζουν αμελητέο variance, και οπότε να σηματοδοτούν πως ίσως να απεικονίζουν θόρυβο που να οδηγούν ενδεχομένως σε overfitting\n",
        "\n",
        "5) Η χρήση dropout ακριβώς πριν την έξοδο, ψαλίδιζε σημαντική πληροφορία σε κρίσιμο σημείο του δικτύου, οπότε η αποφάσεις του μοντέλου μπορεί να παρουσίαζαν τυχαιότητα.\n",
        "\n",
        "6) Η εμπειρική μεθοδολογία αξιολόγησης μοντέλου, όντας τελείως πειραματική, άρα και κάθολου βέλτιστα παραμετροποιημένη, μπορεί να οδήγησε στην επιλογή πολύ χειρότερου μοντέλου από τα διαθέσιμα που εκπαιδεύτηκαν.\n",
        "\n",
        "Ωστόσο έγινε προσπάθεια, σε συνέχεια της παρουσίασης της τελικής εργασίας, να λυθούν αυτά τα προβλήματα σε ξεχωριστό notebook, αλλά κάποια από αυτά συνέχισαν να εμφανίζονται"
      ],
      "metadata": {
        "id": "ewZsEcMQCvQS"
      }
    }
  ]
}